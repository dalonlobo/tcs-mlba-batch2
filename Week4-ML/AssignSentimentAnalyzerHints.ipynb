{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THis notebook contains instruction for both in-class and take home assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1 In class Assignment\n",
    "## Write a simple NB sentiment analyzer. \n",
    "## It will have 2 functions - train, and classify\n",
    "## IT will use the \"DemoClassPreprocess\"class for cleanup of text\n",
    "## I have given the training data for this sample. You need to convert it to appropriate form by calling DemoClassPreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DEMOClassPreprocess as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(labdata):\n",
    "    '''\n",
    "    This function trains all the data fed into labdata\n",
    "    Logic\n",
    "    train for each class. on each feature, if the feature already is present in ModelLabelled increment t\n",
    "    he count by 1/numDocs, else set to 1/NumDocs\n",
    "    This creates the ModelLabelled dictionary object of the kind\n",
    "    {\"word1\": {\"class1\":0.1, \"class2\": 0.3, ..}, \"word2\": {\"class1\": 0.2, \"class2\": 0.3 ....} }\n",
    "    '''\n",
    "    ModelLabelled = {}\n",
    "    #Iterate through all the items in labdata\n",
    "    for 1 in 1:#Complete this for loop\n",
    "        #Find numInp == num sentences in each key and increment that by 1 to avoid diveide by 0\n",
    "        numInp = None\n",
    "        probIncr = round(1.0 / numInp, 9)\n",
    "        ## Now go through all the words, and add to the dictionary for each word, and another dictionary \n",
    "        ## If this word is positive, increment positive value by probIncr\n",
    "        for 1 in 1: # complete this\n",
    "            for 1 in 1: # for all the words\n",
    "                pass\n",
    "    return ModelLabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Complete this function. Given the model formed and a list of words (Bag Of Words) return whether it is positive or negative\n",
    "'''\n",
    "def classify(ModelLabelled, feature):\n",
    "        '''\n",
    "        Given a set of  features identify which class it belongs to\n",
    "        Logic:\n",
    "        For each class,\n",
    "        for each feature, check whether that word is found in the ModelLabelled [class]. If it is \n",
    "        found multiply to previ product, else multiple with a very smallVal called smallVal\n",
    "        which ever class'product is greater the feature belongs to that class\n",
    "        '''\n",
    "        smallVal = 1/1000000000  # any small number\n",
    "        # Result is a dict: {positive: 0.001, negative: 003} This will tell which key is higher.\n",
    "        #Initialize Result = {class1:1, class2: 1, ...}\n",
    "        ##Write Code here\n",
    "        ##Write Code here\n",
    "        for 1 in 1: #iterate through each word in the feature\n",
    "            try:\n",
    "                ##Write Code here\n",
    "                pass\n",
    "                ## Write Code here\n",
    "            except:\n",
    "                ##Write Code here\n",
    "                pass\n",
    "                ## Write Code here\n",
    "        return #The sentiment that has higher score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labdata = {}\n",
    "pre = p.Preprocess() \n",
    "posFeatureArray = \\\n",
    " [\n",
    "                    pre.clean(\"I like this movie ending\").split(\",\"),\n",
    "                    pre.clean(\"I want to go outside as the weather is nice\").split(\",\"),\n",
    "                    pre.clean(\"this movie is apt for kids\").split(\",\"), \n",
    "                    pre.clean(\"the movie is great\").split(\",\")\n",
    " ]\n",
    "negFeatureArray = \\\n",
    " [\n",
    "                    pre.clean(\"I hate this movie\").split(\",\"),\n",
    "                    pre.clean(\"this movie is R Rated and hence unsuitable for kids\").split(\",\"),\n",
    "                    pre.clean(\"working long hours is not necessarily smart\").split(\",\"), \n",
    "                    pre.clean(\"the movie was atrocious\").split(\",\")\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labdata[\"positive\"] = posFeatureArray\n",
    "labdata[\"negative\"] = negFeatureArray\n",
    "ML = train(labdata)\n",
    "test = [\"Movie 2.0 has great special effects\"  , \"In the end all turned out great\"]\n",
    "\n",
    "for t in test:\n",
    "    print (t, pre.clean(t))\n",
    "    print (\" the sentence\", t, \"is\", classify(ML, feature = pre.clean(t).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 2 Take home assignment\n",
    "## Read the data in TweetTrain.csv\n",
    "## For each TweetText column, simplify it by calling pre-process, and build a labdata object as given above.\n",
    "## labdata is a dict containing a list of sentences that are split. It has 2 keys \"positive\", \"negative\" (as given above)\n",
    "## After completing the reading of TweetTrain, consolidate the labdata, and call train function\n",
    "\n",
    "## Then read the data in TweetTest.csv\n",
    "## Create a file called TweetTestOut.csv - should have all columns of TweetTest and \"Predicted Sentiment\"  of negative or positive\n",
    "## for each TweetColumn, split it and predict the sentiment of that sentence and write the result -\"Predicted Sentiment\" to \n",
    "## the Sentiment column of TweetTestOut.csv\n",
    "## Compare the actual Sentiment with the predicted Sentiment\n",
    "\n",
    "##BONUS \n",
    "## Convert the Train and Classify into a  class called SentimentAnalyzer, and call the object'method in the part 2 for \n",
    "## predicting the sentiment of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainFromFile(infile, fields, prep):\n",
    "    fpIn = open (infile, \"r\", errors = \"ignore\")\n",
    "    reader = csv.DictReader(fpIn, fields)\n",
    "    TrainData = {}\n",
    "    TrainData [\"positive\"] = []\n",
    "    TrainData [\"negative\"]=  []\n",
    "    ML = {}\n",
    "    ## Read through each row and construct the TrainData, after cleaning each TweetText  and converting it to list of words\n",
    "    ## and call the Train function that you built\n",
    "    ## Then build a model\n",
    "\n",
    "    return ML, TrainData\n",
    "infile = \"TweetsTrain.csv\"\n",
    "fields = [\"Sentiment\", \"TweetText\"]\n",
    "ML, TrainData = TrainFromFile(infile, fields, pre)\n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now Read the testdata file, and classify each sentiment. Write the result to another file given here\n",
    "def PreprocessFileAndOutput (infile, outfile, fieldsIn, prep, ml):\n",
    "    fpIn = open (infile, \"r\", errors = \"ignore\")\n",
    "    fpOut = open (outfile, \"w\", errors=\"replace\" )\n",
    "    ##write code here \n",
    "\n",
    "    ##write code here \n",
    "    fpIn.close()\n",
    "    fpOut.close()\n",
    "PreprocessFileAndOutput(\"TweetsTest.csv\", \"TweetsTestOut.csv\", fields, pre, ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
